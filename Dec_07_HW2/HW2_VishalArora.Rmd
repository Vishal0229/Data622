---
title: "Data 622 :: HW#2"
author: "Vishal Arora"
date: "12/6/2020"
output:
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 5
  word_document:
    toc: yes
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## PART-A         
**STEP#0 **: Pick any two classifiers of (SVM,Logistic,DecisionTree,NaiveBayes). Pick heart or ecoli dataset. Heart is simpler and ecoli compounds the problem as it is NOT a balanced dataset. From a grading perspective both carry the same weight.                 
**STEP#1 **: For each classifier, Set a seed (43)                
**STEP#2 **: Do a 80/20 split and determine the Accuracy, AUC and as many metrics as returned by the Caret package (confusionMatrix) Call this the base_metric. Note down as best as you can development (engineering) cost as well as computing cost(elapsed time).                                          
Start with the original dataset and set a seed (43). Then run a cross validation of 5 and 10 of the model on the training set. Determine the same set of metrics and compare the cv_metrics with the base_metric. Note down as best as you can development (engineering) cost as well as computing cost(elapsed time).                            
Start with the original dataset and set a seed (43) Then run a bootstrap of 200 resamples and compute the same set of metrics and for each of the two classifiers build a three column table for each experiment (base, bootstrap, cross-validated). Note down as best as you can development (engineering) cost as well as computing cost(elapsed time).                                   


### Load Libraries

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(caret)
library(dplyr)
library(kableExtra)
library(pROC)
library(skimr)
library(ROCR)
library(klaR)
library(randomForest)
library(caret)
```

We will be using Logistic regression and Naive baiyes for Part 1.

```{r}
path<-"C:\\CUNY_AUG27\\DATA622\\heart.csv"
heartDT<-read.csv(path,head=T,sep=',',stringsAsFactors=F)

#Overview of the data
head(heartDT)
dim(heartDT)


#changing the name of first column to age
names(heartDT)[[1]] <- "age"
names(heartDT)
#To check in NA/NaN values are there
sum(is.na(heartDT))

#skimr package is another good way to check descriptive statistics of data.

skimmed_data <- skim(heartDT)
View(skimmed_data)
heartDT$target <- as.factor(heartDT$target)
```
As clearly visible that the age variable is normally distributed. Hence, there is no bias in the data set used.                     

### Splitting data & applying models
```{r}
set.seed(43)
trainidx<-sample(1:nrow(heartDT) , size=round(0.80*nrow(heartDT)),replace=F) 

train_data <- heartDT[trainidx,]

test_data <- heartDT[-trainidx,]

```

### Models

#### Logistic Regression

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Logistic model function
logisticModelCal <- function(tr, trainType){
set.seed(43)
timer <- proc.time()
if(trainType=="base"){
  logModel <- train(form = target ~.,data = train_data,method = "glm", family = "binomial")
  pred <- predict(logModel, newdata=test_data)
  logCM <-confusionMatrix(data=pred, test_data$target)
}else{
  logModel <- train(form = target ~.,data = train_data, trControl=tr, method = "glm", family = "binomial")
  pred <- predict(logModel, newdata=test_data)
  logCM <-confusionMatrix(data=pred, test_data$target)
}

confusMatr <- logCM$table
  #logCM <- confusionMatrix(predict(logModel, subset(test_data, select = -c(target))), test_data$target)
print(logCM)
print(confusMatr)
timer1 <- proc.time()

#metrics
accuracy_LR <- sum(diag(confusMatr)) / sum(confusMatr)
tpr_LR <- confusMatr[1,1]/sum(confusMatr[1,1], confusMatr[2,1])
fpr_LR <- confusMatr[1,2]/sum(confusMatr[1,2], confusMatr[2,2])
tnr_LR <- confusMatr[2,2]/sum(confusMatr[2,2], confusMatr[1,2]) 
fnr_LR <- confusMatr[2,1]/sum(confusMatr[2,1], confusMatr[1,1])
roc_LR <- roc(test_data$target, as.numeric(pred))
auc_LR <- roc_LR$auc
   
computationDelay <- proc.time()- timer1
totalTimeDelay <- proc.time()-timer 

rowResults <- c(round(accuracy_LR,4),round(auc_LR,4),round(tpr_LR,4),round(fpr_LR,4), round(tnr_LR,4),round(fnr_LR,4), round(computationDelay[[3]]), round(totalTimeDelay[[3]],4))
  
return (rowResults)

}

# Logistic Base Model
logBaseResults <- c("Base Logistic metrics",logisticModelCal(trainControl(method = "none"), "base"))
# Logistic Model with CV=5
logCV5Results <- c("CV5 Logistic metrics", logisticModelCal(trainControl(method = "cv", number=5),"cv"))
# Logistic Model with CV=10
logCV10Results <- c("CV10 Logistic metrics",logisticModelCal(trainControl(method = "cv", number=10), "cv"))

```
#### SVM

```{r echo=FALSE, message=FALSE, warning=FALSE}


svmModelcal <- function(tr, trainType){
set.seed(43)
timer <- proc.time()
if(trainType=="base"){
  svmModel <- train(form = target ~.,data = train_data,method = "svmLinear")
  pred <- predict(svmModel, newdata=test_data)
  confuMatrix_svm <-confusionMatrix(data=pred, test_data$target)
}else{
  svmModel <- train(form = target ~.,data = train_data, trControl=tr, method = "svmLinear")
  pred <- predict(svmModel, newdata=test_data)
  confuMatrix_svm <-confusionMatrix(data=pred, test_data$target)
}

print(svmModel)
print(confuMatrix_svm)
timer1 <- proc.time()

#metrics

accuracy_SVM <- sum(diag(confuMatrix_svm$table)) / sum(confuMatrix_svm$table)
tpr_SVM <- confuMatrix_svm$table[1,1]/sum(confuMatrix_svm$table[1,1], confuMatrix_svm$table[2,1])
fpr_SVM <- confuMatrix_svm$table[1,2]/sum(confuMatrix_svm$table[1,2], confuMatrix_svm$table[2,2])
tnr_SVM <- confuMatrix_svm$table[2,2]/sum(confuMatrix_svm$table[2,2], confuMatrix_svm$table[1,2]) 
fnr_SVM <- confuMatrix_svm$table[2,1]/sum(confuMatrix_svm$table[2,1], confuMatrix_svm$table[1,1])
roc_SVM <- roc(test_data$target, as.numeric(pred))
auc_SVM <- roc_SVM$auc

computationDelay <- proc.time()- timer1
totalTimeDelay <- proc.time()-timer 

rowResults <- c(round(accuracy_SVM,4),round(auc_SVM,4),round(tpr_SVM,4),round(fpr_SVM,4), round(tnr_SVM,4),round(fnr_SVM,4), round(computationDelay[[3]],4), round(totalTimeDelay[[3]],4))



return (rowResults)
  
  
}

# Logistic Base Model

tr <- trainControl(method = "none")
svmBaseResults <- c("Base SVM ",svmModelcal(tr, "base"))

tr <- trainControl(method = "cv", number=5)
svmResults5CV <- c("CV5 SVM ",svmModelcal(tr,"cv"))

tr <- trainControl(method = "cv", number=10)
svmResults10CV <- c("CV10 SVM",svmModelcal(tr,"cv"))

```

####  Bootstaping with 200 resamples
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Function for 200 resamples

bootResampling <- function(trainType,tr){
  timer <- proc.time()
  set.seed(43)
  
  train_model = train(
    form = target ~ .,
    data = heartDT,
    trControl = tr,
    method = trainType
    )
    
    
    
    accuracy <- c()
    auc <- c()
    fpr <- c()
    tpr <- c()
    tnr <- c()
    fnr <- c()
   
    i <- 1
    
    timer1 <- proc.time()
    pred_df <- train_model$pred
    for (resample in unique(pred_df$Resample)){
      temp <- filter(pred_df, Resample == resample)
      model_cm <- confusionMatrix(temp$pred, temp$obs)
      accuracy[i] <- model_cm$overall[[1]]
      auc[[i]] <- auc(roc(as.numeric(temp$pred, ordered = TRUE), as.numeric(temp$obs, ordered = TRUE)))
      tpr[[i]] <- model_cm$byClass[[1]]
      fpr[[i]] <- model_cm$byClass[[2]]
      tnr[[i]] <- model_cm$byClass[[5]]
      fnr[[i]] <- model_cm$byClass[[6]]
      i <- i + 1
    }
  
    accuracy <- mean(accuracy)
    auc <- mean(auc)
    tpr <- mean(tpr)
    fpr <- mean(fpr)
    tnr <- mean(tnr)
    fnr <- mean(fnr)
    
    
    computationDelay <- proc.time()- timer1
    
    totalTimeDelay <- proc.time()- timer
    print(train_model)
    
   rowResults <- c(round(accuracy,4),round(auc,4),round(tpr,4),round(fpr,4), round(tnr,4),round(fnr,4), round(computationDelay[[3]],4), round(totalTimeDelay[[3]],4))
}


#
bootLR <- c("LR bootstraping",bootResampling("glm", trainControl(method="boot", number=200, savePredictions = 'final', returnResamp = 'final')))


bootSVM <- c("SVM boostraping",bootResampling("svmLinear", trainControl(method="boot", number=200, savePredictions = 'final', returnResamp = 'final')))

```

#### Result Matrix
```{r echo=FALSE, message=FALSE, warning=FALSE}
resMatrix <- data.frame(matrix(ncol = 9, nrow = 0))

resMatrix <- rbind(resMatrix,logBaseResults,logCV5Results,logCV10Results,svmBaseResults, svmResults5CV,svmResults10CV, bootLR, bootSVM)
colnames(resMatrix) <- c("ALGO", "AUC","ACC", "TPR", "FPR", "TNR ", "FNR","ComputationTime" ,"TotalTimeDelay")

kable(resMatrix) %>% 
  kable_styling(bootstrap_options = c("striped","hover","condensed","responsive"),full_width   = F,position = "left",font_size = 12) %>%
  row_spec(0, background ="gray") 
```
## Part B                          

### Random Forest 

Creating a baseline for comparison by using the recommend defaults for each parameter and mtry=floor(sqrt(ncol(x)))

```{r message=FALSE, warning=FALSE}
#	
# Create model with default paramters
timer <- proc.time()
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
set.seed(43)
mtry <- sqrt(ncol(train_data))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(target~., data=heartDT, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
totalTimeDelay <- (proc.time()- timer)[[3]]

print(paste0("Total time delay for default random forest :",totalTimeDelay))
print(rf_default)


```

#### Random Search        

Below model will generate 15 random values of mtry at each time tunning. We have 15 values because of tunning length is 15.
```{r  message=FALSE, warning=FALSE}
timer <- proc.time()
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
ntree <- 3
set.seed(43)
#Random generate 15 mtry values with tuneLength = 15
mtry <- sqrt(ncol(train_data))
rf_random <- train(target~., data=heartDT, method="rf", metric=metric, tuneLength=15, trControl=control)
totalTimeDelay <- (proc.time()- timer)[[3]]
print(paste0("Total time delay for default random forest :",totalTimeDelay))
print(rf_default) 

```

#### Grid Search                      
Create control function for training with 10 folds and keep 3 folds for training. search method is grid.
```{r message=FALSE, warning=FALSE}
set.seed(43)
timer <- proc.time()
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3, 
                        search='grid')
#create tunegrid with 15 values from 1:15 for mtry to tunning model. Our train function will change number of entry variable at each split according to tunegrid. 
tunegrid <- expand.grid(.mtry = (1:15)) 

rf_gridsearch <- train(target ~ ., 
                       data = heartDT,
                       method = 'rf',
                       metric = 'Accuracy',
                       tuneGrid = tunegrid)
totalTimeDelay <- (proc.time()- timer)[[3]]

print(paste0("Total time delay for default random forest :",totalTimeDelay))
print(rf_gridsearch)

```


Manual tunning  approach create many model caret scenarios with different manual parameters and compare its accuracy. We do this to evaluate different ntree while hodling mtry constant.
```{r message=FALSE, warning=FALSE}
set.seed(43)
timer <- proc.time()
control <- trainControl(method = 'repeatedcv',
                        number = 10,
                        repeats = 3,
                        search = 'grid')
#create tunegrid
tunegrid <- expand.grid(.mtry = c(sqrt(ncol(train_data))))
modellist <- list()

#train with different ntree parameters
for (ntree in c(1000,1500,2000,2500)){
 
  fit <- train(target~.,
               data = heartDT,
               method = 'rf',
               metric = 'Accuracy',
               tuneGrid = tunegrid,
               trControl = control,
               ntree = ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}
totalTimeDelay <- (proc.time()- timer)[[3]]
#Compare results
results <- resamples(modellist)

print(paste0("Total time delay for default random forest :",totalTimeDelay))
print(summary(results))

 

```

## Part C

### Conclusion : 

As is clearly visible from the result matrix for Part A , the Accuracy and other performance indicators are nearly same for the Base Model and CV Models for Logistic Regression & SVM. But it is the total time delay which is different for all the models. Between CV and Bootstraping  models interms of Accuracy nothing much to choose for but it is the Total time taken by Bootstraping models which makes them more expensive in terms of computing resources.

Pareto's rule was implemented by implementing 80/20 rule while splitting the data and also the 20 % of our data is more critical in testing that our models are not overfitting or underfitting.

Occam's razor is the principle that, of two explanations that account for all the facts, the simpler one is more likely to be correct. In out case as Accuracy is not significant in case of Part A models and in terms of Total time delay also the difference is in seconds/milliseconds . So as per Ocam's razor principle we should go with Base Models (LR or SVM).

However Random Forest model with grid search has the best Accuracy with 83% , but if we tune our models manually then the accuracy can vary between 71% to 97% .But Random Forest model are very time consuming and require lot of computational power. 